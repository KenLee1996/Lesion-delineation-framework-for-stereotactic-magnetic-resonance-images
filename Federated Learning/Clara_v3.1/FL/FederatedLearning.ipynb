{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Federated Learning with Clara Train SDK",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Medical data is sensitive and needs to be protected. And even after anonymization processes, \nit is often infeasible to collect and share patient data from several institutions in a centralised data lake. \nThis poses challenges for training machine learning algorithms, such as deep convolutional networks, \nwhich require extensive and balanced data sets for training and validation.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Federated learning (FL) is a learning paradigm that sidesteps this difficulty: \ninstead of pooling the data, the machine learning process is executed locally at each participating institution and only intermediate model training updates are shared among them. \nIt thereby allows to train algorithms collaboratively without exchanging the underlying datasets and neatly addresses the problem of data governance and privacy that arise when pooling medical data. \nThere are different FL communication architectures, such as the Client-server approach via hub and spokes, a decentralized architecture via peer-to-peer or hybrid variants. \n\nThe FL tool in the Clara Train SDK is a client-server architecture, \nin which a federated server manages the aggregation and distribution as shown below.\n\u003cbr\u003e![fl](screenShots/FLDiagram.png)\u003cbr\u003e \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Prerequisites\n- None. This note book explains FL.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Resources\n- Watch talk covering Clara Train SDK basics [S22563](https://developer.nvidia.com/gtc/2020/video/S22563)\nClara train Getting started: cover basics, BYOC, AIAA, AutoML \n- GTC 2020 talk [Federated Learning for Medical Imaging: Collaborative AI without Sharing Patient Data](https://developer.nvidia.com/gtc/2020/video/s21536-vid)\n- [Federated learning blog](https://blogs.nvidia.com/blog/2019/10/13/what-is-federated-learning/)\n- [Federated learning blog at RSNA](https://blogs.nvidia.com/blog/2019/12/01/clara-federated-learning/)  \n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Overview",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Federated Learning in Clara Train SDK uses a client-server architecture. \nThe image below gives you an overview. \nFor details about the components, please see our [documentation](https://docs.nvidia.com/clara/tlt-mi/clara-train-sdk-v3.0/nvmidl/additional_features/federated_learning.html?highlight\u003dfederated).  \n\nThe key things to note are:\n* A server is responsible for **managing training, keeping best model and aggregating gradients**.\n* Clients are responsible for **training local model** and sending updates (gradients) to server.\n* **No data from the dataset is shared** between clients or with server.\n* To ensure **privacy**, all communication with server is secured.\n* Additional privacy-preserving mechanisms can be enabled.\n\nFigure below shows these concepts and how they are communicated to the server.\n\u003cbr\u003e![fl](screenShots/FLDetails.png)\u003cbr\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Server",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The following diagram shows the server workflow:\n\n\u003cbr\u003e![fl](screenShots/fl_server_workflow.png)\u003cbr\u003e",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "A federated server is responsible for:\n1. Initialising a global model at federated round 0\n1. Sharing the global model with all clients\n1. Synchronising model updates from multiple clients\n1. Updating the global model when sufficient model updates received\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "##  Client",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The following diagram shows the client workflow:\n\u003cbr\u003e![fl2](screenShots/fl_client_workflow.png)\u003cbr\u003e",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "A federated client will:\n1. Download the global model\n1. Train the model with local training data\n1. Upload `delta_w` (the difference between the updated model and the global model) to the server\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "# FL Challenges \nIn order to run a federated learning experiment, you need think about:\n1. Software development:\n    1. Security:\n        1. Secure connection\n        2. Authentication\n        3. Certification ssl\n    2. Deadlocks: Clients join / die / re-join\n    4. Unstable client server connection\n    5. Scaling: How to run large FL experiment with 20 or 100 clients\n    6. With different sites having different data size, how to enable local training with multiple GPUs, also how to give weights to different clients.\n    7. Audit trails: clients need to know who did what, when    \n\n2. Logistics: \u003cspan style\u003d\"color:red\"\u003e(Most challenging)\u003c/span\u003e\n\n    1. FL experiment is typically conducted through multiple experiments to tune hyper parameters. \n    How to synchronize these runs\n    2. Keep track of experiments across sites.\n    3. FL most important feature is to improve the off diagonal metric. \n    Clients would share results (validation metric) and NOT the data. \n    \u003cspan style\u003d\"color:red\"\u003e This is the hardest to do \u003c/span\u003e since you need to distribute the models from each client to the rest.  \n\n3. Research:\n    1. How to aggregate model weights from each site\n    2. Privacy for your model weight sharing \n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Clara software engineers have taken care of the 1st and 2nd bullets for you so researchers can focus on the 3rd bullet.\nMoreover, In Clara train V3.1, FL comes with a new provisioning tool what simplifies the process.\n\nLets start by defining some terminologies used throughout FL discussion:\n\n- __Study__: An FL project with preset goals (e.g. train the EXAM model) and identified participants.\n- __Org__: The organization that participates in the study.\n- __Site__: The computing system that runs FL application as part of the study.\nThere are two kinds of sites: Server and Clients.\nEach client belongs to an organization.\n- __Provisioning Tool__: The tool used for provisioning all participants of the study.\n- __FL Server__: An application responsible for client coordination based on FL federation rules and model aggregation.\n- __FL Client__: An application running on a client site that performs model training with its local datasets and collaborates with the FL Server for federated study.\n- __Admin Client__: An application running on a userâ€™s machine that allows the user to perform FL system operations with a command line interface.\n- __Lead IT__: The person responsible for provisioning the participants and coordinating IT personnel from all sites for the study.\nThe Lead IT is also responsible for the management of the Server.\n- __Site IT__: The person responsible for the management of the Site of his/her organization.\n- __Lead Researcher__: The scientist who works with Site Scientists to ensure the success of the study.\n- __Site Researcher__: The scientist who works with the Lead Scientist to make sure the Site is properly prepared for the study.\n\nNOTE: in certain projects, a person could play several of the above-mentioned roles. \n\u003cbr\u003e![fl](screenShots/FLWorkFlow.png)\u003cbr\u003e",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Diagram above shows high level steps of an FL study:  \n1. Lead IT configures everything in a config.yaml file, runs provisioning which will generate zip packages for each client.\n2. These packages contains everything a FL clients needs from starting the docker, ssl certification, etc to start and complete the FL experiment.\n3. Each client starts the docker and the FL client use the provided Startup Kit.\n4. Similarly the FL server starts the docker and FL server using the provided Startup Kit.\n5. Finally the Admin can either use docker or pip install the admin tool, which will connect to the server and start the FL experiment\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## With this in mind we have created 3 sub-notebooks: \n1. [Provisioning](Provisioning.ipynb) which walks you though the configurations you set and how to run the tool\n2. [Client](Client.ipynb) walks you through a FL client\n3. [Admin](Admin.ipynb) walks you through how the FL admin data scientist would conduct the FL experiment once the server and clients are up and running\n ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "\u003c!--- SPDX-License-Identifier: Apache-2.0 --\u003e\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}